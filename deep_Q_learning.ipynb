{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import math\n",
    "import scipy\n",
    "import scipy.interpolate\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import Model\n",
    "class PositionEncoding1D(Layer):#not using this rn\n",
    "    def __init__(self,output_dim, input_dim):\n",
    "        super(PositionEncoding1D,self).__init__()\n",
    "        self.encoded_pos=tf.zeros([input_dim,output_dim])\n",
    "        pos=tf.cast(tf.expand_dims(tf.range(input_dim),1),tf.dtypes.float32)\n",
    "        div_term = tf.exp((tf.cast(tf.range(0, output_dim, 2),tf.dtypes.float32) *\n",
    "                         -(np.log(10000.0) / output_dim)))\n",
    "        #interleave the arrays with deep tensorflow magick\n",
    "        #https://stackoverflow.com/questions/46431983/concatenate-two-tensors-in-alternate-fashion-tensorflow\n",
    "        sin_term=tf.expand_dims(tf.sin(pos*div_term),1)\n",
    "        cos_term=tf.expand_dims(tf.sin(pos*div_term),1)\n",
    "        both_terms=tf.concat([sin_term, cos_term],2)\n",
    "        self.encoded_pos = tf.reshape(both_terms, [-1,output_dim])\n",
    "    def call(self, tensor):\n",
    "        return tf.math.reduce_sum((tf.expand_dims(tensor,-1)*self.encoded_pos),axis=-2)\n",
    "class GameState:\n",
    "    def __init__(self,m,n,initial_state=None,num_shuffles=100):\n",
    "        self.m=m\n",
    "        self.n=n\n",
    "        \n",
    "        transition_map=[[1],[n],[-n],[-1]]\n",
    "        #self.transition_map=[x+y for x in transition_map for y in transition_map if x[-1]!=-y[0]]\n",
    "        #self.transition_map=[x+y for x in self.transition_map for y in transition_map if x[-1]!=-y[0]]\n",
    "        self.transition_map=transition_map\n",
    "        self.state=None\n",
    "        if initial_state is None:\n",
    "            L=np.zeros(m*n)\n",
    "            ix=0\n",
    "            for i in range(m):\n",
    "                for j in range(n):\n",
    "                    if ix<m*n-1:\n",
    "                        L[i*n+j]=ix+1\n",
    "                        ix+=1\n",
    "            self.state=L\n",
    "            self.scramble_state(num_shuffles)\n",
    "        else:\n",
    "            self.state=initial_state\n",
    "    def scramble_state(self, num_shuffles):\n",
    "        for random_move in range(num_shuffles):\n",
    "            self.transition(random.randrange(len(self.transition_map)))\n",
    "    def slide(self,action):\n",
    "        place1=np.where(self.state==0)[0][0]\n",
    "        place2=place1+action\n",
    "        if place2<0 or place2>self.m*self.n-1:\n",
    "            place2=place1-action\n",
    "        self.state[[place1,place2]]=self.state[[place2,place1]]\n",
    "    def transition(self,index):\n",
    "        for action in self.transition_map[index]:\n",
    "            self.slide(action)\n",
    "    def score(self):\n",
    "        error_by_cell=self.state[:-1]-np.arange(1,self.m*self.n)\n",
    "        return len(np.where(error_by_cell==0)[0])\n",
    "    def is_complete(self):\n",
    "        return self.score()==len(self.state)-1\n",
    "    def partial_score(self):\n",
    "        i15=np.where(self.state==self.m*self.n-1)[0][0]\n",
    "        i1=np.where(self.state==1)[0][0]\n",
    "        return (i15==len(self.state)-2)  and (i1==0) \n",
    "        \n",
    "def groupoid_basis(p):\n",
    "    amn=scipy.linalg.circulant(np.eye(3)[0])\n",
    "    return scipy.linalg.block_diag(*tuple(amn) * p ** 2)\n",
    "    \n",
    "class GroupoidDecompositionLayer(Layer):\n",
    "    def __init__(self,m,n,name=None,trainable=True,dtype='float32'):\n",
    "        super(GroupoidDecompositionLayer,self).__init__(name=name,trainable=True)\n",
    "        self.basis=groupoid_basis(2)\n",
    "        self.m=m\n",
    "        self.n=n\n",
    "    def call(self,tensor):\n",
    "        return tf.linalg.trace(tf.matmul(tensor, self.basis))\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"m\": self.m,\n",
    "            \"n\": self.n\n",
    "        })\n",
    "        return config\n",
    "class PermutationEquivariantLayer(Layer):\n",
    "    def __init__(self, output_dim, name=None,trainable=True,dtype='float32'):\n",
    "        super(PermutationEquivariantLayer,self).__init__(name=name,trainable=True)\n",
    "        w_init = tf.random_uniform_initializer(-1e-7,1e-7)\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=w_init(shape=(output_dim,2), dtype=dtype),\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.output_dim=output_dim\n",
    "    def call(self,tensor):\n",
    "        x1=tf.expand_dims(tensor,-1)*tf.transpose(self.w[:,0])\n",
    "        x2=tf.expand_dims(tensor,-1)*self.w[:,1]\n",
    "        return tf.math.reduce_sum(tf.math.reduce_sum(tf.expand_dims(x1,-2)+tf.expand_dims(x2,-1),-1),1)\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class SlideAgent:\n",
    "    def __init__(self, \n",
    "                 m, \n",
    "                 n, \n",
    "                 exploration_probability=0.9,\n",
    "                 exploration_decay=0.015,\n",
    "                 gamma=0.9, \n",
    "                 num_trials_per_update=5, \n",
    "                 max_turns_per_game=50, \n",
    "                 max_cycle_penalty=0.00004,\n",
    "                 experience_replay_sample_size=128,\n",
    "                 learning_rate=0.001,\n",
    "                 num_hidden_units=4,\n",
    "                print_every_step=False):\n",
    "        self.num_trials_per_update=num_trials_per_update\n",
    "        self.max_turns_per_game=max_turns_per_game\n",
    "        self.max_cycle_penalty=max_cycle_penalty\n",
    "        self.print_every_step=print_every_step\n",
    "        self.experience_replay_sample_size=experience_replay_sample_size\n",
    "        self.m=m\n",
    "        self.n=n\n",
    "        self.gamma=gamma\n",
    "        self.exploration_probability=exploration_probability\n",
    "        self.exploration_decay=exploration_decay\n",
    "        self.num_policies=len(GameState(m,n).transition_map)\n",
    "        self.input_size=self.m*self.n*2\n",
    "        input_layer=Input(shape=(self.input_size,))\n",
    "        reshaped=Reshape([self.input_size,1])(input_layer)\n",
    "        attention=Attention(use_scale=True)([reshaped,reshaped])\n",
    "        attention=Reshape([self.input_size])(attention)\n",
    "        concat=Concatenate()([attention, input_layer])\n",
    "        hidden_layers=[]\n",
    "        for layer in range(num_hidden_units):\n",
    "            hidden_layers.append(LeakyReLU()(PermutationEquivariantLayer(self.input_size)(concat)))\n",
    "            concat=sum(hidden_layers)\n",
    "        representation_layer=PermutationEquivariantLayer(144)(concat)\n",
    "        representation_layer=Reshape([12,12])(representation_layer)\n",
    "        output_layer=GroupoidDecompositionLayer(self.m,self.n)(representation_layer)\n",
    "        output_layer=output_layer\n",
    "        self.experience_replay_buffer=[np.zeros([0,self.input_size],dtype=np.float32),np.array([],dtype=np.float32)]\n",
    "        self.dqn=Model(inputs=[input_layer], outputs=[output_layer])\n",
    "        self.dqn.compile(loss='huber', optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "        custom_objects={'PermutationEquivariantLayer': PermutationEquivariantLayer,\n",
    "                        'GroupoidDecompositionLayer': GroupoidDecompositionLayer}\n",
    "        tf.keras.utils.get_custom_objects().update(custom_objects)\n",
    "        self.dqn_target=tf.keras.models.clone_model(self.dqn)\n",
    "        self.dqn_target.trainable=False\n",
    "        self.update_target()\n",
    "    def update_target(self):\n",
    "        self.dqn_target.set_weights(self.dqn.get_weights())\n",
    "    def choose_policy(self,state,target=False):\n",
    "        q_values=[]\n",
    "        state=np.repeat(np.reshape(state,[1,self.m*self.n]),self.num_policies,axis=0)\n",
    "        policies=np.arange(self.num_policies)\n",
    "        q_values=self.predict_q(state,policies,target).numpy()\n",
    "        choice=np.argmax(q_values.reshape([-1,self.num_policies]))\n",
    "        return choice, q_values[choice]\n",
    "    def predict_q(self,state,policy, target=False):\n",
    "        model_input_vector=tf.convert_to_tensor(self.encode_input(state,policy).reshape([-1,self.input_size]))\n",
    "        if target:\n",
    "            return tf.stop_gradient(self.dqn_target(model_input_vector))\n",
    "        else:\n",
    "            return self.dqn(model_input_vector)\n",
    "    def encode_input(self,state,policy):\n",
    "        S=np.zeros([state.shape[0],state.shape[1]*2])\n",
    "        for i in range(state.shape[0]):\n",
    "            g=GameState(self.m,self.n,initial_state=state[i])\n",
    "            g.transition(policy[i])\n",
    "            state2=g.state\n",
    "            S[i]=np.concatenate([state[i],state2])\n",
    "        model_input_vector=S\n",
    "        return model_input_vector\n",
    "    def reward(self,game,replay,policy):\n",
    "        score_adjusted= 0\n",
    "        reward_lut=[0,0,0.1,0.1,0.5,0.9,1,1,1,1,10,10,80,100,175]\n",
    "        lut_indices=np.linspace(0,self.m*self.n-1,len(reward_lut))\n",
    "        reward_fn=scipy.interpolate.interp1d(lut_indices,reward_lut,kind='nearest')\n",
    "        if game.is_complete():\n",
    "            score_adjusted=1.0/(self.m*self.n)\n",
    "        elif game.partial_score():\n",
    "            score_adjusted+=reward_fn(game.score())/(self.m*self.n*10000)\n",
    "        if not game.is_complete():\n",
    "            for i,move in enumerate(replay[:-1]):\n",
    "                if np.all(game.state == move[0]):\n",
    "                    score_adjusted-=self.max_cycle_penalty*(0.1+1/(len(replay)-i)) #penalize periodic behavior, especially short cycles\n",
    "        return score_adjusted\n",
    "    def play_game(self,shuffles):\n",
    "        replay=[]\n",
    "        game=GameState(self.m,self.n,None,shuffles)\n",
    "        while game.is_complete():\n",
    "            game=GameState(self.m,self.n,None,shuffles) #just in case\n",
    "        for iteration in range(self.max_turns_per_game):\n",
    "            if np.random.rand()<self.exploration_probability:\n",
    "                policy=random.randrange(0,self.num_policies)\n",
    "            else:\n",
    "                policy,Q=self.choose_policy(game.state,target=False)\n",
    "            game.transition(policy)\n",
    "            Q1=self.choose_policy(game.state.reshape([1,-1]),target=True)[1]\n",
    "            replay.append([game.state,policy,self.reward(game,replay,policy)])\n",
    "            if self.print_every_step:\n",
    "                print(f'{np.reshape(game.state,[self.m,self.n])}'.replace(' 0',' _').replace('[0','[_'))\n",
    "            if game.is_complete():\n",
    "                break\n",
    "            if len(replay)>3 and np.all(game.state==-replay[-3][0]): #we've just got ourselves stuck in a period-2 orbit\n",
    "                break\n",
    "        if self.print_every_step:\n",
    "            print(replay[-1][-1])\n",
    "        return replay,game.is_complete(), game.score()\n",
    "    \n",
    "    def update_weights(self,inputs,rewards):\n",
    "        Q1=np.array([])\n",
    "        for state in inputs[:,self.m*self.n:]:\n",
    "            Q1=np.concatenate([Q1,self.choose_policy(state,target=True)[1]*np.ones(1)])\n",
    "        loss=self.dqn.train_on_batch(tf.convert_to_tensor(inputs), tf.convert_to_tensor(rewards+self.gamma*Q1))\n",
    "        return loss\n",
    "            \n",
    "    def train_once(self,epoch):\n",
    "        transition_batch=[]\n",
    "        state_batch=[]\n",
    "        reward_batch=[]\n",
    "        scores=[]\n",
    "        win_rate=0\n",
    "        difficulty=min(self.max_turns_per_game,int(np.sqrt(epoch)*0.05+5))\n",
    "        difficulty0=min(self.max_turns_per_game,int(np.sqrt(epoch)*0.05+5))\n",
    "        if difficulty0<difficulty:\n",
    "            self.exploration_probability=max(self.exploration_probability,0.125)\n",
    "        for trial in range(self.num_trials_per_update):\n",
    "            replay_buffer, won, score=self.play_game(difficulty)\n",
    "            state_batch+=[x[0] for x in replay_buffer]\n",
    "            transition_batch+=[x[1] for x in replay_buffer]\n",
    "            reward_batch+=[x[-1] for x in replay_buffer]\n",
    "            scores.append(score)\n",
    "            if won:\n",
    "                win_rate+=1/self.num_trials_per_update\n",
    "        batch=self.encode_input(np.array(state_batch),np.array(transition_batch, dtype=np.int32))\n",
    "        self.experience_replay_buffer[0]=np.concatenate([self.experience_replay_buffer[0], batch],axis=0)\n",
    "        self.experience_replay_buffer[1]=np.concatenate([self.experience_replay_buffer[1], np.array(reward_batch,dtype=np.float32)])\n",
    "        loss=None\n",
    "        if self.experience_replay_buffer[0].shape[0]>=self.experience_replay_sample_size:   \n",
    "            experience_sample=np.random.choice(np.arange(self.experience_replay_buffer[0].shape[0]),self.experience_replay_sample_size)\n",
    "            inputs=tf.convert_to_tensor(self.experience_replay_buffer[0][experience_sample], dtype=tf.float32)\n",
    "            rewards=tf.convert_to_tensor(self.experience_replay_buffer[1][experience_sample], dtype=tf.float32)\n",
    "            loss=self.update_weights(inputs,rewards)\n",
    "        self.exploration_probability=(1-self.exploration_decay)*self.exploration_probability\n",
    "        mean_score=np.mean(scores)\n",
    "        return win_rate, np.array(reward_batch), loss, mean_score, difficulty\n",
    "        \n",
    "agent=SlideAgent(4,4,print_every_step=False)\n",
    "print(agent.dqn.summary())\n",
    "iteration=0\n",
    "losses=[]\n",
    "while True:\n",
    "    win_rate, rewards, loss, mean_score, difficulty=agent.train_once(iteration+1)\n",
    "    iteration+=1\n",
    "    losses.append(loss)\n",
    "    if iteration%5==0:\n",
    "        print(f'win rate:{win_rate} on iteration {iteration}, loss: {loss}, mean score: {mean_score}, difficulty: {difficulty}')\n",
    "        agent.update_target()\n",
    "    if iteration%20==0:\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(np.arange(len(rewards)), rewards)\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.scatter(np.arange(len(losses)), losses)\n",
    "        plt.show()\n",
    "    if iteration%100==0:\n",
    "        agent.dqn.save('dqn')\n",
    "        agent.dqn_target.save('dqn_target')\n",
    "        #open(f'agent_{iteration}.pkl','wb+').write(pickle.dumps(agent))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
